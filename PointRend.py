# -*- coding: utf-8 -*-
"""“PointRend in Detectron2 Tutorial.ipynb”的副本

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jhf793tCVq_SBZju4iagwpo_Gkj549uE

# "[PointRend](https://arxiv.org/abs/1912.08193) in Detectron2" Tutorial

<img src="https://dl.fbaipublicfiles.com/detectron2/Detectron2-Logo-Horz.png" width="500">

Welcome to the [PointRend project](https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend) in detectron2! In this tutorial, we will go through some basics usage of PointRend, including the following:
* Run inference on images or videos, with an existing PointRend model
* Look into PointRend internal representation.

You can make a copy of this tutorial or use "File -> Open in playground mode" to play with it yourself.

# Install detectron2
"""

# install dependencies: 
#####!pip install pyyaml==5.1
# check pytorch installation: 
import torch, torchvision
print(torch.__version__, torch.cuda.is_available())

# clone the repo in order to access pre-defined configs in PointRend project
#####!git clone --branch v0.6 https://github.com/facebookresearch/detectron2.git detectron2_repo
# install detectron2 from source
#####!pip install -e detectron2_repo
# See https://detectron2.readthedocs.io/tutorials/install.html for other installation options

#!pip uninstall detectron2
#!pip install detectron2

# You may need to restart your runtime prior to this, to let your installation take effect
# Some basic setup:
# Setup detectron2 logger
import detectron2
from detectron2.utils.logger import setup_logger
setup_logger()

# import some common libraries
import numpy as np
import cv2
import torch
#from google.colab.patches import cv2_imshow

# import some common detectron2 utilities
from detectron2 import model_zoo
from detectron2.engine import DefaultPredictor
from detectron2.config import get_cfg
from detectron2.utils.visualizer import Visualizer, ColorMode
from detectron2.data import MetadataCatalog
coco_metadata = MetadataCatalog.get("coco_2017_val")

# import PointRend project
from detectron2.projects import point_rend

"""# Run a pre-trained PointRend model

We first download an image from the COCO dataset:
"""


import os
import argparse

# Instantiate the parser
parser = argparse.ArgumentParser(description='Process some inputs.')

# Add argument
parser.add_argument('--input_folder', type=str,
                    help='An input folder for processing',default="/scratch/bbsb/xu10/pointrend-demo/inputs/camel-full")

# Parse the arguments
args = parser.parse_args()

# Assign the passed argument to input_folder variable
input_folder = args.input_folder # could be './inputs/camel-480p/'
'''
# List all files in the directory
input_folder = './inputs/camel-480p/'
'''
files = os.listdir(input_folder)

# Sort the files so they are processed in order
files.sort()
def point_rend_main(im): 
    cfg = get_cfg()
    cfg.merge_from_file(model_zoo.get_config_file("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml"))
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
    cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml")
    mask_rcnn_predictor = DefaultPredictor(cfg)
    mask_rcnn_outputs = mask_rcnn_predictor(im)

    """Now, we load a PointRend model and show its prediction."""

    cfg = get_cfg()
    # Add PointRend-specific config
    point_rend.add_pointrend_config(cfg)
    # Load a config from file
    cfg.merge_from_file("detectron2_repo/projects/PointRend/configs/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco.yaml")
    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5  # set threshold for this model
    # Use a model from PointRend model zoo: https://github.com/facebookresearch/detectron2/tree/master/projects/PointRend#pretrained-models
    cfg.MODEL.WEIGHTS = "detectron2://PointRend/InstanceSegmentation/pointrend_rcnn_R_50_FPN_3x_coco/164955410/model_final_edd263.pkl"
    predictor = DefaultPredictor(cfg)
    outputs = predictor(im)

    # Show and compare two predictions: 
    v = Visualizer(im[:, :, ::-1], coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)
    mask_rcnn_result = v.draw_instance_predictions(mask_rcnn_outputs["instances"].to("cpu")).get_image()
    v = Visualizer(im[:, :, ::-1], coco_metadata, scale=1.2, instance_mode=ColorMode.IMAGE_BW)
    point_rend_result = v.draw_instance_predictions(outputs["instances"].to("cpu")).get_image()
    print("Mask R-CNN with PointRend (top)     vs.     Default Mask R-CNN (bottom)")
    #####cv2.imshow(np.concatenate((point_rend_result, mask_rcnn_result), axis=0)[:, :, ::-1])
    cv2.imwrite(input_folder+file+"_01Mask R-CNN with PointRend (top)     vs.     Default Mask R-CNN (bottom).png", np.concatenate((point_rend_result, mask_rcnn_result), axis=0)[:, :, ::-1])

    """# Visualize PointRend point sampling process

    In this section we show how PointRend's point sampling process works. To do this, we need to access intermediate representations of the predictor `model.forward(...)` function. Thus, we run forward step manually by accessing the internal states & attributes of the model.
    """

    # First we define a simple function to help us plot the intermediate representations.
    import matplotlib.pyplot as plt

    def plot_mask(mask, title="", point_coords=None, figsize=10, point_marker_size=5):
      '''
      Simple plotting tool to show intermediate mask predictions and points 
      where PointRend is applied.
      
      Args:
        mask (Tensor): mask prediction of shape HxW
        title (str): title for the plot
        point_coords ((Tensor, Tensor)): x and y point coordinates
        figsize (int): size of the figure to plot
        point_marker_size (int): marker size for points
      '''

      H, W = mask.shape
      plt.figure(figsize=(figsize, figsize))
      if title:
        title += "_ "
      plt.title("{}resolution {}x{}".format(title, H, W), fontsize=30)
      plt.ylabel(H, fontsize=30)
      plt.xlabel(W, fontsize=30)
      plt.xticks([], [])
      plt.yticks([], [])
      plt.imshow(mask, interpolation="nearest", cmap=plt.get_cmap('gray'))
      if point_coords is not None:
        plt.scatter(x=point_coords[0], y=point_coords[1], color="red", s=point_marker_size, clip_on=True) 
      plt.xlim(-0.5, W - 0.5)
      plt.ylim(H - 0.5, - 0.5)
      plt.savefig("{}.png".format(title))# added 2023-6-2
      plt.show()

    """With `predictor` and `im` loaded in the previous section we run backbone, bounding box prediction, and coarse mask segmenation head. We visualize mask prediction for the foreground plane on the image."""

    from detectron2.data import transforms as T
    model = predictor.model
    # In this image we detect several objects but show only the first one.
    instance_idx = 0
    # Mask predictions are class-specific, "plane" class has id 4.
    category_idx = 4

    #with torch.no_grad():#commented by Pengcheng 2023-6-23
    if True:
      # Prepare input image.
      height, width = im.shape[:2]
      im_transformed = T.ResizeShortestEdge(800, 1333).get_transform(im).apply_image(im)
      
      #####batched_inputs = [{"image": torch.as_tensor(im_transformed).permute(2, 0, 1)}] #originall
      batched_inputs = [{"image": torch.as_tensor(im_transformed.copy()).permute(2, 0, 1)}]#modified 2023-6-2

      # Get bounding box predictions first to simplify the code.
      detected_instances = [x["instances"] for x in model.inference(batched_inputs)]
      [r.remove("pred_masks") for r in detected_instances]  # remove existing mask predictions
      pred_boxes = [x.pred_boxes for x in detected_instances] 

      # Run backbone.
      images = model.preprocess_image(batched_inputs)
      features = model.backbone(images.tensor)
      ############################
      print("model",model)
      print("model.backbone",model.backbone)
      for k,v in features.items():
          print("*"*100,f"features-{k}.shape",v.shape)
      print("model.roi_heads",model.roi_heads)
      # Given the bounding boxes, run coarse mask prediction head.
      mask_coarse_logits = model.roi_heads.mask_head.coarse_head(model.roi_heads.mask_head._roi_pooler(features, pred_boxes))

      plot_mask(
          mask_coarse_logits[instance_idx, category_idx].cpu().detach().numpy(),#.to("cpu"),
          title=input_folder+file+"_02Coarse prediction"
      )

    # Prepare features maps to use later
    mask_features_list = [
      features[k] for k in model.roi_heads.mask_head.mask_point_in_features
    ]
    features_scales = [
      model.roi_heads.mask_head._feature_scales[k] 
      for k in model.roi_heads.mask_head.mask_point_in_features
    ]
    print("mask_features_list",mask_features_list)
    print("features_scales",features_scales)

    """### Point sampling during training

    During training we select points where coarse prediction is uncertain to train PointRend head. See section 3.1 in the PointRend [paper](https://arxiv.org/abs/1912.08193) for more details.

    To visualize different sampling strategy change `oversample_ratio` and `importance_sample_ratio` parameters below.
    """

    from detectron2.projects.point_rend.mask_head import calculate_uncertainty
    from detectron2.projects.point_rend.point_features import get_uncertain_point_coords_with_randomness

    # Change number of points to select
    num_points = 14 * 14
    # Change randomness parameters 
    oversample_ratio = 3  # `k` in the paper
    importance_sample_ratio = 0.75  # `\beta` in the paper

    #with torch.no_grad():
    if True: #commented by Pengcheng 2023-6-23
      # We take predicted classes, whereas during real training ground truth classes are used.
      pred_classes = torch.cat([x.pred_classes for x in detected_instances])

      # Select points given a corse prediction mask
      point_coords = get_uncertain_point_coords_with_randomness(
        mask_coarse_logits,
        lambda logits: calculate_uncertainty(logits, pred_classes),
        num_points=num_points,
        oversample_ratio=oversample_ratio,
        importance_sample_ratio=importance_sample_ratio
      )

      H, W = mask_coarse_logits.shape[-2:]
      plot_mask(
        mask_coarse_logits[instance_idx, category_idx].cpu().detach().numpy(),#.to("cpu"),
        title=input_folder+file+"_03Sampled points over the coarse prediction",
        point_coords=(
          W * point_coords[instance_idx, :, 0].to("cpu") - 0.5,
          H * point_coords[instance_idx, :, 1].to("cpu") - 0.5
        ),
        point_marker_size=50
      )

    """### Point sampling during inference

    Starting from a 7x7 coarse prediction we bilinearly upsample it `num_subdivision_steps` times. At each step we find `num_subdivision_points` most uncertain points and make predictions for them using the PointRend head. See section 3.1 in the [paper](https://arxiv.org/abs/1912.08193) to know more details.

    Change `num_subdivision_steps` and `num_subdivision_points` parameters to change inference behavior.
    """

    from detectron2.layers import interpolate
    from detectron2.projects.point_rend.mask_head import calculate_uncertainty
    from detectron2.projects.point_rend.point_features import (
        get_uncertain_point_coords_on_grid,
        point_sample,
        point_sample_fine_grained_features,
    )

    num_subdivision_steps = 5
    num_subdivision_points = 28 * 28


    #with torch.no_grad():#commented by Pengcheng 2023-6-23
    if True:
      plot_mask(
          mask_coarse_logits[0, category_idx].cpu().detach().numpy(),#.to("cpu").numpy(), 
          title=input_folder+file+"_04Coarse prediction"
      )

      mask_logits = mask_coarse_logits
      for subdivions_step in range(num_subdivision_steps):
        # Upsample mask prediction
        mask_logits = interpolate(
            mask_logits, scale_factor=2, mode="bilinear", align_corners=False
        )
        # If `num_subdivision_points` is larger or equalt to the
        # resolution of the next step, then we can skip this step
        H, W = mask_logits.shape[-2:]
        if (
          num_subdivision_points >= 4 * H * W
          and subdivions_step < num_subdivision_steps - 1
        ):
          continue
        # Calculate uncertainty for all points on the upsampled regular grid
        uncertainty_map = calculate_uncertainty(mask_logits, pred_classes)
        # Select most `num_subdivision_points` uncertain points
        point_indices, point_coords = get_uncertain_point_coords_on_grid(
            uncertainty_map, 
            num_subdivision_points
        )

        # Extract fine-grained and coarse features for the points
        fine_grained_features, _ = point_sample_fine_grained_features(
          mask_features_list, features_scales, pred_boxes, point_coords
        )
        coarse_features = point_sample(mask_coarse_logits, point_coords, align_corners=False)

        # Run PointRend head for these points
        point_logits = model.roi_heads.mask_head.point_head(fine_grained_features, coarse_features)

        # put mask point predictions to the right places on the upsampled grid.
        R, C, H, W = mask_logits.shape
        x = (point_indices[instance_idx] % W).to("cpu")
        y = (point_indices[instance_idx] // W).to("cpu")
        point_indices = point_indices.unsqueeze(1).expand(-1, C, -1)
        mask_logits = (
          mask_logits.reshape(R, C, H * W)
          .scatter_(2, point_indices, point_logits)
          .view(R, C, H, W)
        )
        plot_mask(
          mask_logits[instance_idx, category_idx].cpu().detach().numpy(),#.to("cpu"), 
          title=input_folder+file+"_05Subdivision step: {}".format(subdivions_step + 1),
          point_coords=(x, y)
        )

    """We can visualize mask prediction obtained in the previous block."""

    from detectron2.modeling import GeneralizedRCNN
    from detectron2.modeling.roi_heads.mask_head import mask_rcnn_inference

    results = detected_instances
    mask_rcnn_inference(mask_logits, results)
    results = GeneralizedRCNN._postprocess(results, batched_inputs, images.image_sizes)[0]

    # We can use `Visualizer` to draw the predictions on the image.
    v = Visualizer(im_transformed[:, :, ::-1], coco_metadata)
    v = v.draw_instance_predictions(results["instances"].to("cpu"))
    ######cv2.imshow(v.get_image()[:, :, ::-1])
    cv2.imwrite(input_folder+file+"_06Visualizer to draw the predictions on the image.png", v.get_image()[:, :, ::-1])
    return mask_logits
      
      
      

# Process each file
for file in files:
    # Ensure the file is a .jpg before processing
    if file.endswith('00000.jpg'):
        # Create a complete file path
        file_path = os.path.join(input_folder, file)
        
        # Load the image
        im = cv2.imread(file_path)
        point_rend_main(im)
